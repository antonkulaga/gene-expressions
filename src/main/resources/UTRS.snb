{"content":{"metadata":{"name":"UTRS","user_save_timestamp":"1970-01-01T03:00:00.000Z","auto_save_timestamp":"1970-01-01T03:00:00.000Z","language_info":{"name":"scala","file_extension":"scala","codemirror_mode":"text/x-scala"},"trusted":true,"customLocalRepo":null,"customRepos":null,"customDeps":["\"org.bdgenomics.adam\" % \"adam-core\" % \"0.16.0\"","\"org.bdgenomics.bdg-formats\" % \"bdg-formats\" % \"0.5.0\""],"customImports":[],"customSparkConf":{"spark.app.name":"ADAM","spark.master":"spark://antonkulaga:7077","spark.executor.memory":"6G","spark.worker.memory":"6G","spark.serializer":"org.apache.spark.serializer.KryoSerializer","spark.kryo.registrator":"org.bdgenomics.adam.serialization.ADAMKryoRegistrator","spark.kryoserializer.buffer.mb":"4","spark.kryo.referenceTracking":"true"}},"cells":[{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"import scala.io.Source\nimport org.apache.spark._\nimport org.bdgenomics.adam.rdd.ADAMContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.projections._\nimport org.bdgenomics.adam.rdd.features._\nimport org.bdgenomics.formats.avro._\nimport org.bdgenomics.adam.models._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.bdgenomics.adam.rdd._\nimport org.bdgenomics.adam.rdd.features.{GTFParser, FeatureParser}\n\n\n@transient def sc = sparkContext\n@transient lazy val ac = new ADAMContext(sc)\n\nval path = \"/home/antonkulaga/data/\"\nval samplesPath = path+ \"samples/\"\nval utrs = path + \"utrs/\"\nval destination = utrs + \"output/\"\nval reference = path + \"Drosophila_melanogaster.BDGP5.76.gtf\"","outputs":[{"name":"stdout","output_type":"stream","text":"import scala.io.Source\nimport org.apache.spark._\nimport org.bdgenomics.adam.rdd.ADAMContext\nimport org.bdgenomics.adam.rdd.ADAMContext._\nimport org.bdgenomics.adam.projections._\nimport org.bdgenomics.adam.rdd.features._\nimport org.bdgenomics.formats.avro._\nimport org.bdgenomics.adam.models._\nimport org.apache.spark.rdd.RDD\nimport org.apache.spark.SparkContext\nimport org.bdgenomics.adam.rdd._\nimport org.bdgenomics.adam.rdd.features.{GTFParser, FeatureParser}\nsc: org.apache.spark.SparkContext\nac: org.bdgenomics.adam.rdd.ADAMContext = <lazy>\npath: String = /home/antonkulaga/data/\nsamplesPath: String = /home/antonkulaga/data/samples/\nutrs: String = /home/antonkulaga/data/utrs/\ndestination: String = /home/antonkulaga/data/utrs/output/\nreference: String = /home/antonkulaga/data/Drosophila_mel..."},{"metadata":{},"data":{"text/html":"/home/antonkulaga/data/Drosophila_melanogaster.BDGP5.76.gtf"},"output_type":"execute_result","execution_count":2}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"def loadFeatures(filePath:String) = ac.loadFeatures(filePath).map(f =>  \n    if(f.getFeatureType==\"exon\" && f.getFeatureId==null) {\n      val ats = f.getAttributes\n      f.setFeatureId(ats.get(\"exon_number\")+\"-\"+ats.get(\"transcript_id\"))\n      f\n    }else f)\n\n/**\n   * Saves only gene features\n   * @param filePath\n   * @param destination\n   */\n  def saveGenesOnly(filePath:String,destination:String) = {\n    val features: RDD[Feature] =   loadFeatures(filePath).filter(f=>f.getFeatureType==\"gene\")\n    features.adamParquetSave(destination)\n  }\n\n  def convertGTF(filePath:String,destination: String) = {\n    val features: RDD[Feature] = loadFeatures(filePath)\n    features.adamParquetSave(destination)\n  }\n\n  def convertGTF(filePath:String,destination: String, geneFeatures:RDD[Feature]) = {\n    val features: RDD[Feature] =  loadFeatures(filePath) ++ geneFeatures\n    features.adamParquetSave(destination)\n  }\n\n  def cutGenes(filePath:String,destination:String,ids:Set[String]) = loadFeatures(filePath)\n      .filter{   case f=> ids.contains(f.getAttributes.get(\"gene_id\"))     }\n      .adamParquetSave(destination)","outputs":[{"name":"stdout","output_type":"stream","text":"loadFeatures: (filePath: String)org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature]\nsaveGenesOnly: (filePath: String, destination: String)Unit\nconvertGTF: (filePath: String, destination: String)Unit <and> (filePath: String, destination: String, geneFeatures: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature])Unit\nconvertGTF: (filePath: String, destination: String)Unit <and> (filePath: String, destination: String, geneFeatures: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature])Unit\ncutGenes: (filePath: String, destination: String, ids: Set[String])Unit\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":22}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"val gf = loadFeatures(destination+\"genes.adam\").cache()\nconvertGTF(samplesPath+\"3.gtf\",destination+\"features_3.adam\",gf)\nconvertGTF(samplesPath+\"4.gtf\",destination+\"features_4.adam\",gf)\nconvertGTF(samplesPath+\"9.gtf\",destination+\"features_9.adam\",gf)\n","outputs":[{"name":"stdout","output_type":"stream","text":"gf: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = MappedRDD[11] at map at <console>:106\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":18}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"val ids =  sc.textFile(utrs+\"ids.txt\").collect().toSet\ncutGenes(destination+\"features_3.adam\",destination+\"features_small_3.adam\",ids)\ncutGenes(destination+\"features_4.adam\",destination+\"features_small_4.adam\",ids)\ncutGenes(destination+\"features_9.adam\",destination+\"features_small_9.adam\",ids)","outputs":[{"name":"stdout","output_type":"stream","text":"ids: scala.collection.immutable.Set[String] = Set(FBgn0264270, FBgn0264502, FBgn0264386, FBgn0030278, FBgn0064912, FBgn0086681, FBgn0035538, FBgn0050274, FBgn0052274, \"\", FBgn0023416, FBgn0051816, FBgn0026206, FBgn0030840, FBgn0051740, FBgn0262477, FBgn0041094, FBgn0035823, FBgn0016080, FBgn0264794, FBgn0011281, FBgn0263331, FBgn0025616, FBgn0037464, FBgn0040735, FBgn0033863, FBgn0067903, FBgn0036346, FBgn0263111, FBgn0053462, FBgn0039367, FBgn0036731, FBgn0031545, FBgn0052371, FBgn0039064, FBgn0085422, FBgn0052064, FBgn0029859, FBgn0264490, FBgn0263597, FBgn0035453, FBgn0260454, FBgn0032464, FBgn0085233, FBgn0259795, FBgn0034082, FBgn0038837, FBgn0265627, FBgn0013301, FBgn0250840, FBgn0052643, FBgn0265683, FBgn0034713, FBgn0260657, FBgn0015230, FBgn0052459, FBgn0267726, FBgn0052086, FB..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":23}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"def loadTranscripts(filePath:String) = ac.loadFeatures(filePath).filter(f=>f.getFeatureType==\"transcript\")\ndef loadUTRs(filePath:String) = ac.loadFeatures(filePath).filter(f=>f.getFeatureType==\"UTR\")\ndef loadGenes(filePath:String) = ac.loadFeatures(filePath).filter(f=>f.getFeatureType==\"gene\")\n\nval (t3,t4,t9) = (\nloadTranscripts(destination+\"features_small_3.adam\"),\nloadTranscripts(destination+\"features_small_4.adam\"),\nloadTranscripts(destination+\"features_small_9.adam\")\n  )\nval (g3,g4,g9) = (\nloadGenes(destination+\"features_small_3.adam\"),\nloadGenes(destination+\"features_small_4.adam\"),\nloadGenes(destination+\"features_small_9.adam\")\n  )\n\nval (u3,u4,u9) = (\nloadUTRs(destination+\"features_small_3.adam\"),\nloadUTRs(destination+\"features_small_4.adam\"),\nloadUTRs(destination+\"features_small_9.adam\")  \n)\n","outputs":[{"name":"stdout","output_type":"stream","text":"loadTranscripts: (filePath: String)org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature]\nloadUTRs: (filePath: String)org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature]\nloadGenes: (filePath: String)org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature]\nt3: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = FilteredRDD[600] at filter at <console>:150\nt4: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = FilteredRDD[603] at filter at <console>:150\nt9: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = FilteredRDD[606] at filter at <console>:150\ng3: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = FilteredRDD[609] at filter at <console>:152\ng4: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature] = Fil..."},{"metadata":{},"data":{"text/html":"FilteredRDD[624] at filter at &lt;console&gt;:151"},"output_type":"execute_result","execution_count":46}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"import org.bdgenomics.adam.predicates.HighQualityReadPredicate\ndef strand(str: Strand): Boolean = str match {\n  case Strand.Forward     => true\n  case Strand.Reverse     => false\n  case Strand.Independent => true\n}\n\n\nlazy val fields = Seq(AlignmentRecordField.readName,\n  AlignmentRecordField.readMapped,\n  AlignmentRecordField.contig,\n  AlignmentRecordField.primaryAlignment,\n  AlignmentRecordField.start,\n  AlignmentRecordField.end,\n  AlignmentRecordField.mapq,\n  AlignmentRecordField.sequence\n)\nlazy val projection =Projection(fields)\nval reads = \"/home/antonkulaga/data/adam/\"\n\ndef load(filePath:String): RDD[AlignmentRecord] =  ac.loadAlignments(filePath,predicate = Some(classOf[HighQualityReadPredicate]),projection =Some(projection))\n\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.bdgenomics.adam.predicates.HighQualityReadPredicate\nstrand: (str: org.bdgenomics.formats.avro.Strand)Boolean\nfields: Seq[org.bdgenomics.adam.projections.AlignmentRecordField.SchemaVal] = <lazy>\nprojection: org.apache.avro.Schema = <lazy>\nreads: String = /home/antonkulaga/data/adam/\nload: (filePath: String)org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord]\n"},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":34}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"import org.bdgenomics.adam.rich._\nimport ReferenceMappingContext._\nimport org.bdgenomics.adam.rdd._\n\n  def join(features:RDD[Feature],reads:RDD[AlignmentRecord]): RDD[(Feature, AlignmentRecord)] = {\n    import scala.reflect._\n    BroadcastRegionJoin.partitionAndJoin(sc,features,reads)(\n      org.bdgenomics.adam.rich.ReferenceMappingContext.FeatureReferenceMapping,\n      org.bdgenomics.adam.rich.ReferenceMappingContext.AlignmentRecordReferenceMapping,classTag[Feature],classTag[AlignmentRecord])\n  }\n\n  def counts(features:RDD[Feature],reads:RDD[AlignmentRecord]): RDD[(String, Int)] = {\n    join(features,reads).groupBy{  case (f,a)=>f.getFeatureId  }.map{\n      case (f,i)=>f->i.size\n    }\n  }\n\n  def countsByGeneIds(features:RDD[Feature],reads:RDD[AlignmentRecord]): RDD[(String, Int)] = {\n    join(features,reads).groupBy{  case (f,a)=>f.getAttributes.get(\"gene_id\")  }.map{\n      case (f,i)=>f->i.size\n    }\n  }\n\n  def countsByGeneIdsAndLen(features:RDD[Feature],reads:RDD[AlignmentRecord]): RDD[((String, Long), Int)] = {\n    join(features,reads).groupBy{  case (f,a)=>(f.getAttributes.get(\"gene_id\"),Math.abs(f.getEnd-f.getStart))  }.map{\n      case (f,i)=>f->i.size\n    }\n  }\n\n  def saveCounts(features:RDD[Feature],reads:RDD[AlignmentRecord],filePath:String): RDD[(String, Int)] = {\n    val cs = counts(features,reads)\n    cs.saveAsTextFile(filePath)\n    cs\n  }\n\n  def saveCountsByGeneIdsAndLen(features:RDD[Feature],reads:RDD[AlignmentRecord],filePath:String): RDD[((String, Long), Int)] = {\n    val cs = countsByGeneIdsAndLen(features,reads)\n    cs.saveAsTextFile(filePath)\n    cs\n  }\n","outputs":[{"name":"stdout","output_type":"stream","text":"import org.bdgenomics.adam.rich._\nimport ReferenceMappingContext._\nimport org.bdgenomics.adam.rdd._\njoin: (features: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature], reads: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord])org.apache.spark.rdd.RDD[(org.bdgenomics.formats.avro.Feature, org.bdgenomics.formats.avro.AlignmentRecord)]\ncounts: (features: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature], reads: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord])org.apache.spark.rdd.RDD[(String, Int)]\ncountsByGeneIds: (features: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.Feature], reads: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord])org.apache.spark.rdd.RDD[(String, Int)]\ncountsByGeneIdsAndLen: (..."},{"metadata":{},"data":{"text/html":""},"output_type":"execute_result","execution_count":49}]},{"metadata":{"trusted":true,"collapsed":false},"cell_type":"code","source":"val (r3,r4,r9) = (load(reads+\"3.adam\"),load(reads+\"4.adam\"),load(reads+\"9.adam\"))\nval cs3 = saveCountsByGeneIdsAndLen(u3,r3,destination+\"counts_UTRs_3.txt\")\n","outputs":[{"name":"stdout","output_type":"stream","text":"r3: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord] = FilteredRDD[652] at filter at ADAMContext.scala:176\nr4: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord] = FilteredRDD[655] at filter at ADAMContext.scala:176\nr9: org.apache.spark.rdd.RDD[org.bdgenomics.formats.avro.AlignmentRecord] = FilteredRDD[658] at filter at ADAMContext.scala:176\ncs3: org.apache.spark.rdd.RDD[((String, Long), Int)] = MappedRDD[673] at map at <console>:165\n"},{"metadata":{},"data":{"text/html":"MappedRDD[673] at map at &lt;console&gt;:165"},"output_type":"execute_result","execution_count":50}]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"","outputs":[]}],"nbformat":4},"name":"UTRS","path":"UTRS.snb","writable":true}